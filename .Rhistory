set.seed(42)
ggplot(cleaned_tweets, aes(label = word, size = n)) +
geom_text_wordcloud_area(rm_outside = T) +
scale_size_area(max_size = 16) +
theme_fivethirtyeight()
immigrants <- (search_tweets(q = "immigrants, rats", n = 3200, include_rts = FALSE, lang = "en"))
immigrants <- (search_tweets(q = "immigrants, rats", n = 3200, include_rts = FALSE, lang = "en"))
# Select relevant columns
immigrants<- immigrants %>%
separate(created_at, into = c("date", "time"), sep = " ")%>%
select(date, time, screen_name, text, favorite_count, retweet_count, hashtags)
head(immigrants)
# Clean-up associated with social media data
# Remove URLs
immigrants$text <- gsub("http.*","", immigrants$text)
immigrants$text <- gsub("https.*","", immigrants$text)
immigrants$text <- gsub("t.co.*","", immigrants$text)
immigrants$text <- gsub("amp.*","", immigrants$text)
# we need to restructure it as one-token-per-row format
tidy_tweets <- immigrants %>%
unnest_tokens(word, text)
cleaned_tweets <- tidy_tweets %>%
anti_join(get_stopwords())
cleaned_tweets<- cleaned_tweets %>%
dplyr::count(word, sort = TRUE)%>%
top_n(100)%>%
filter(word != "immigrants" | word != "rats") %>%
mutate(word= reorder(word, n))
set.seed(42)
ggplot(cleaned_tweets, aes(label = word, size = n)) +
geom_text_wordcloud_area(rm_outside = T) +
scale_size_area(max_size = 16) +
theme_fivethirtyeight()
immigrants <- (search_tweets(q = "immigrants, rats", n = 3200, include_rts = FALSE, lang = "en"))
View(immigrants)
immigrants <- (search_tweets(q = "immigrants", n = 3200, include_rts = FALSE, lang = "en"))
immigrants<- immigrants %>%
separate(created_at, into = c("date", "time"), sep = " ")%>%
select(date, time, screen_name, text, favorite_count, retweet_count, hashtags)
head(immigrants)
# Remove URLs
immigrants$text <- gsub("http.*","", immigrants$text)
immigrants$text <- gsub("https.*","", immigrants$text)
immigrants$text <- gsub("t.co.*","", immigrants$text)
immigrants$text <- gsub("amp.*","", immigrants$text)
# we need to restructure it as one-token-per-row format
tidy_tweets <- immigrants %>%
unnest_tokens(word, text)
cleaned_tweets <- tidy_tweets %>%
anti_join(get_stopwords())
cleaned_tweets<- cleaned_tweets %>%
dplyr::count(word, sort = TRUE)%>%
top_n(70)%>%
filter(word != "immigrants") %>%
mutate(word= reorder(word, n))
View(cleaned_tweets)
set.seed(42)
ggplot(cleaned_tweets, aes(label = word, size = n)) +
geom_text_wordcloud_area(rm_outside = T) +
scale_size_area(max_size = 16) +
theme_fivethirtyeight()
?scale_color_gradient
immigrants <- (search_tweets(q = "immigrants", n = 3200, include_rts = FALSE, lang = "en"))
immigrants<- immigrants %>%
separate(created_at, into = c("date", "time"), sep = " ")%>%
select(date, time, screen_name, text, favorite_count, retweet_count, hashtags)
head(immigrants)
# Remove URLs
immigrants$text <- gsub("http.*","", immigrants$text)
immigrants$text <- gsub("https.*","", immigrants$text)
immigrants$text <- gsub("t.co.*","", immigrants$text)
immigrants$text <- gsub("amp.*","", immigrants$text)
# we need to restructure it as one-token-per-row format
tidy_tweets <- immigrants %>%
unnest_tokens(word, text)
cleaned_tweets <- tidy_tweets %>%
anti_join(get_stopwords())
cleaned_tweets<- cleaned_tweets %>%
dplyr::count(word, sort = TRUE)%>%
top_n(100)%>%
filter(word != "immigrants") %>%
mutate(word= reorder(word, n))
set.seed(42)
ggplot(cleaned_tweets, aes(label = word, size = n)) +
geom_text_wordcloud_area(rm_outside = T) +
scale_size_area(max_size = 16) +
theme_fivethirtyeight()+
scale_colour_gradient2()
ggplot(cleaned_tweets, aes(label = word, size = n, color = word)) +
geom_text_wordcloud_area(rm_outside = T) +
scale_size_area(max_size = 16) +
theme_fivethirtyeight()+
scale_colour_gradient2()
install.packages("colourpicker")
library(colourpicker)
plotHelper()
function (string, start = 1L, end = start, sep = fixed(" "))
ggplot(cleaned_tweets, aes(label = word, size = n, color = word)) +
geom_text_wordcloud_area(rm_outside = T) +
scale_size_area(max_size = 16) +
theme_fivethirtyeight()+
scale_colour_gradient(low = "maroon", high = "goldenrod1")
set.seed(42)
ggplot(cleaned_tweets, aes(label = word, size = n, color = word)) +
geom_text_wordcloud_area(rm_outside = T) +
scale_size_area(max_size = 16) +
theme_fivethirtyeight()+
scale_colour_gradient(low = "maroon", high = "goldenrod1")
ggplot(cleaned_tweets, aes(label = word, size = n, color = word)) +
geom_text_wordcloud_area(rm_outside = T) +
scale_size_area(max_size = 16) +
theme_fivethirtyeight()+
scale_colour_gradient(low = "darkred", high = "red")
ggplot(cleaned_tweets, aes(label = word, size = n)) +
geom_text_wordcloud_area(rm_outside = T) +
scale_size_area(max_size = 16) +
theme_fivethirtyeight()+
scale_colour_gradient(low = "darkred", high = "red")
ggplot(cleaned_tweets, aes(label = word, size = n, colour = n)) +
geom_text_wordcloud_area(rm_outside = T) +
scale_size_area(max_size = 16) +
theme_fivethirtyeight()+
scale_colour_gradient(low = "darkred", high = "red")
ggplot(cleaned_tweets, aes(label = word, size = n, colour = n)) +
geom_text_wordcloud_area(rm_outside = T, eccentricity = 1) +
scale_size_area(max_size = 20) +
theme_fivethirtyeight()+
scale_colour_gradient(low = "maroon", high = "goldenrod1")
ggplot(cleaned_tweets, aes(label = word, size = n, colour = n)) +
geom_text_wordcloud_area(rm_outside = T, eccentricity = 1) +
scale_size_area(max_size = 30) +
theme_fivethirtyeight()+
scale_colour_gradient(low = "maroon", high = "goldenrod1")
ggplot(cleaned_tweets, aes(label = word, size = n, colour = n)) +
geom_text_wordcloud_area(rm_outside = T, eccentricity = 1.5) +
scale_size_area(max_size = 30) +
theme_fivethirtyeight()+
scale_colour_gradient(low = "maroon", high = "goldenrod1")
library(wesanderson)
library(RColorBrewer)
immigrants <- (search_tweets(q = "immigrants", n = 3200, include_rts = FALSE, lang = "en"))
immigrants<- immigrants %>%
separate(created_at, into = c("date", "time"), sep = " ")%>%
select(date, time, screen_name, text, favorite_count, retweet_count, hashtags)
head(immigrants)
# Remove URLs
immigrants$text <- gsub("http.*","", immigrants$text)
immigrants$text <- gsub("https.*","", immigrants$text)
immigrants$text <- gsub("t.co.*","", immigrants$text)
immigrants$text <- gsub("amp.*","", immigrants$text)
# we need to restructure it as one-token-per-row format
tidy_tweets <- immigrants %>%
unnest_tokens(word, text)
cleaned_tweets <- tidy_tweets %>%
anti_join(get_stopwords())
cleaned_tweets<- cleaned_tweets %>%
dplyr::count(word, sort = TRUE)%>%
top_n(150)%>%
filter(word != "immigrants") %>%
mutate(word= reorder(word, n))
pal <- wes_palette("Zissou1", 150, type = "continuous")
set.seed(42)
ggplot(cleaned_tweets, aes(label = word, size = n, colour = n)) +
geom_text_wordcloud_area(rm_outside = T, eccentricity = 1.5) +
scale_size_area(max_size = 30) +
theme_fivethirtyeight()+
scale_fill_manual(values = pal)
pal
pal <- wes_palette("Zissou1", 10, type = "continuous")
set.seed(42)
ggplot(cleaned_tweets, aes(label = word, size = n, colour = n)) +
geom_text_wordcloud_area(rm_outside = T, eccentricity = 1.5) +
scale_size_area(max_size = 30) +
theme_fivethirtyeight()+
scale_fill_manual(values = pal)
ggplot(cleaned_tweets, aes(label = word, size = n, colour = n)) +
geom_text_wordcloud_area(rm_outside = T, eccentricity = 1.5) +
scale_size_area(max_size = 20) +
scale_radius(range = c(0, 20), limits = c(0, NA)) +
theme_fivethirtyeight()+
scale_fill_manual(values = pal)
ggplot(cleaned_tweets, aes(label = word, size = n, colour = n)) +
geom_text_wordcloud_area(rm_outside = T, eccentricity = 1.5) +
scale_radius(range = c(0, 20), limits = c(0, NA)) +
theme_fivethirtyeight()+
scale_fill_manual(values = pal)
ggplot(cleaned_tweets, aes(label = word, size = n, colour = n)) +
geom_text_wordcloud_area(rm_outside = T, eccentricity = 1.5) +
scale_radius(range = c(0, 30), limits = c(0, NA)) +
theme_fivethirtyeight()+
scale_fill_manual(values = pal)
View(immigrants)
View(tidy_tweets)
View(cleaned_tweets)
refugees <- (search_tweets(q = "refugees", n = 3200, include_rts = FALSE, lang = "en"))
refugees<- refugees %>%
separate(created_at, into = c("date", "time"), sep = " ")%>%
select(date, time, screen_name, text, favorite_count, retweet_count, hashtags)
head(refugees)
# Remove URLs
refugees$text <- gsub("http.*","", refugees$text)
refugees$text <- gsub("https.*","", refugees$text)
refugees$text <- gsub("t.co.*","", refugees$text)
refugees$text <- gsub("amp.*","", refugees$text)
# we need to restructure it as one-token-per-row format
tidy_tweets <- refugees %>%
unnest_tokens(word, text)
cleaned_tweets <- tidy_tweets %>%
anti_join(get_stopwords())
cleaned_tweets<- cleaned_tweets %>%
dplyr::count(word, sort = TRUE)%>%
top_n(150)%>%
filter(word != "immigrants") %>%
mutate(word= reorder(word, n))
ggplot(cleaned_tweets, aes(label = word, size = n, colour = n)) +
geom_text_wordcloud_area(rm_outside = T, eccentricity = 1.5) +
scale_radius(range = c(0, 30), limits = c(0, NA)) +
theme_fivethirtyeight()+
scale_color_gradient(low = "darkred", high = "red")+
ggplot(cleaned_tweets, aes(label = word, size = n, colour = n)) +
geom_text_wordcloud_area(rm_outside = T, eccentricity = 1.5) +
scale_size_area(max_size = 20) +
theme_fivethirtyeight()+
scale_color_gradient(low = "darkred", high = "red")
View(cleaned_tweets)
library(tidyverse)
library(tidyverse)
n_obs <- 50
intercept <- 0.5
betas <- c(1, 0.25, 0.25)
sigmas <- c(1, 1, 0.25)
data1 <- tibble(
grp = rep(c(-0.5, 0.5), each=n_obs),
y = rep(c(intercept, intercept+betas[1]), each=n_obs) + rnorm(n_obs*2, 0, sigmas[1])
)
data2 <- tibble(
grp = rep(c(-0.5, 0.5), each=n_obs),
y = rep(c(intercept, intercept+betas[2]), each=n_obs) + rnorm(n_obs*2, 0, sigmas[2])
)
data3 <- tibble(
grp = rep(c(-0.5, 0.5), each=n_obs),
y = rep(c(intercept, intercept+betas[3]), each=n_obs) + rnorm(n_obs*2, 0, sigmas[3])
)
View(data1)
View(data2)
View(data3)
install.packages("brms")
library(brms)
m1 <- brm(y ~ grp, data = data1)
m2 <- brm(y ~ grp, data = data2)
m3 <- brm(y ~ grp, data = data3)
?brms::plot.brmsfit
plot(m1, combo=c("areas", "trace"), pars="grp")
plot(m1)
plot(m1, pars = "grp")
plot(m1, pars = "grp", combo = ("areas"))
plot(m1, pars = "grp", combo = ("areas"."trace"))
plot(m1, pars = "grp", combo = ("areas","trace"))
plot(m1, pars = "grp", combo = ("areas","trace"))
?brms::plot.brmsfit
plot(m1, pars = "grp", combo = c("areas", "trace"))
plot(m1, pars = "grp", combo = c("dens", "trace"))
plot(m1, pars = "grp", combo = c("violin", "trace"))
plot(m1, pars = "grp", combo = c("intervals", "trace"))
plot(m1, pars = "grp", combo = c("areas", "trace"))
plot(m1, pars = "grp", combo = c("areas_ridges", "trace"))
plot(m2, pars = "grp", combo = c("areas_ridges", "trace"))
plot(m3, pars = "grp", combo = c("areas_ridges", "trace"))
bootstrap_permute <- function(dat) {
dat %>%
# a) bootstrap: sample rows with replacement
slice_sample(replace=TRUE, prop=1) %>%
# b) permute: sample grp without replacement
mutate(grp = sample(grp))
}
bsp_data1 <- replicate(500, bootstrap_permute(data1), simplify=FALSE)
dat %>%
# a) bootstrap: sample rows with replacement
dplyr::slice_sample(replace=TRUE, prop=1) %>%
# b) permute: sample grp without replacement
mutate(grp = sample(grp))
bootstrap_permute <- function(dat) {
dat %>%
# a) bootstrap: sample rows with replacement
dplyr::slice_sample(replace=TRUE, prop=1) %>%
# b) permute: sample grp without replacement
mutate(grp = sample(grp))
}
bsp_data1 <- replicate(500, bootstrap_permute(data1), simplify=FALSE)
library(dplyr)
install.packages("dplyr")
install.packages("dplyr")
install.packages("dplyr")
install.packages("dplyr")
library(dplyr)
remove.packages("dplyr")
install.packages("dplyr")
install.packages("dplyr")
library(dplyr)
library(tidyverse)
library(dplyr)
bootstrap_permute <- function(dat) {
dat %>%
# a) bootstrap: sample rows with replacement
slice_sample(replace=TRUE, prop=1) %>%
# b) permute: sample grp without replacement
mutate(grp = sample(grp))
}
library(brms)
m1 <- brm(y ~ grp, data = data1)
data1 <- tibble(
grp = rep(c(-0.5, 0.5), each=n_obs),
y = rep(c(intercept, intercept+betas[1]), each=n_obs) + rnorm(n_obs*2, 0, sigmas[1])
)
data2 <- tibble(
grp = rep(c(-0.5, 0.5), each=n_obs),
y = rep(c(intercept, intercept+betas[2]), each=n_obs) + rnorm(n_obs*2, 0, sigmas[2])
)
data3 <- tibble(
grp = rep(c(-0.5, 0.5), each=n_obs),
y = rep(c(intercept, intercept+betas[3]), each=n_obs) + rnorm(n_obs*2, 0, sigmas[3])
)
n_obs <- 50
intercept <- 0.5
betas <- c(1, 0.25, 0.25)
sigmas <- c(1, 1, 0.25)
data1 <- tibble(
grp = rep(c(-0.5, 0.5), each=n_obs),
y = rep(c(intercept, intercept+betas[1]), each=n_obs) + rnorm(n_obs*2, 0, sigmas[1])
)
data2 <- tibble(
grp = rep(c(-0.5, 0.5), each=n_obs),
y = rep(c(intercept, intercept+betas[2]), each=n_obs) + rnorm(n_obs*2, 0, sigmas[2])
)
data3 <- tibble(
grp = rep(c(-0.5, 0.5), each=n_obs),
y = rep(c(intercept, intercept+betas[3]), each=n_obs) + rnorm(n_obs*2, 0, sigmas[3])
)
m1 <- brm(y ~ grp, data = data1)
m2 <- brm(y ~ grp, data = data2)
m3 <- brm(y ~ grp, data = data3)
plot(m1, pars = "grp", combo = c("areas_ridges", "trace"))
plot(m2, pars = "grp", combo = c("areas_ridges", "trace"))
plot(m3, pars = "grp", combo = c("areas_ridges", "trace"))
bsp_data1 <- replicate(500, bootstrap_permute(data1), simplify=FALSE)
bsp_data2 <- replicate(500, bootstrap_permute(data2), simplify=FALSE)
bsp_data3 <- replicate(500, bootstrap_permute(data3), simplify=FALSE)
bsp_m1 <- lapply(bsp_data1, function(dat_i) update(m1, newdata=dat_i))
bsp_m2 <- lapply(bsp_data2, function(dat_i) update(m2, newdata=dat_i))
bsp_m3 <- lapply(bsp_data3, function(dat_i) update(m3, newdata=dat_i))
install.packages(c("ggdist", "ggstance"))
install.packages(c("ggdist", "ggstance"))
library(ggdist)
install.packages("tidyverse")
install.packages("tidyverse")
library(ggdist)
library(ggplot2)
library(ggdist)
sessionInfo()
library(ggdist)
library(ggridges)
library(ggstance)
remove.packages("ggdist")
install.packages("ggdist")
library(ggdist)
library(ggplot2)
library(ggdist)
remove.packages("tidyverse")
install.packages("tidyverse")
install.packages("tidyverse")
library(tidyverse)
library(ggdist)
Ms <- list(m1, m2, m3)
bsp_Ms <- list(bsp_m1, bsp_m2, bsp_m3)
res <- map_df(1:3, function(data_nr) {
# get the true data's posterior samples
post_long <- posterior_samples(Ms[[data_nr]]) %>%
pivot_longer(c(b_Intercept, b_grp), names_to="param", values_to="value") %>%
mutate(src = "True Posterior Samples")
# get the posterior samples from all models fit to bootstrapped-permuted data
bsp_post_long <- map_df(1:length(bsp_Ms[[data_nr]]), function(bsp_nr) {
bsp_Ms[[data_nr]][[bsp_nr]] %>%
posterior_samples() %>%
select(b_Intercept, b_grp) %>%
mutate(bsp_nr = bsp_nr)
}) %>%
pivot_longer(c(b_Intercept, b_grp), names_to="param", values_to="value") %>%
mutate(src = "Bootstrapped Permutation Posterior Samples")
# combine into one dataframe
bind_rows(post_long, bsp_post_long) %>%
mutate(df=sprintf("Model fit to data%g", data_nr))
})
res %>%
ggplot(aes(value, param, fill=src, colour=src)) +
geom_density_ridges(alpha=0.2, colour=NA) +
stat_pointinterval(.width=0.89, position=position_dodgev(height = 0.1)) +
labs(
x = "Value", y = "Parameter", fill="Source", colour="Source",
title="Bootstrap Permutation Posterior Samples ROPE"
) +
facet_wrap(vars(df), scales="free_x") +
theme(legend.position = "bottom")
res %>%
ggplot(aes(value, param, fill=src, colour=src)) +
geom_density_ridges(alpha=0.2, colour=NA) +
# stat_pointinterval(.width=0.89, position=position_dodgev(height = 0.1)) +
labs(
x = "Value", y = "Parameter", fill="Source", colour="Source",
title="Bootstrap Permutation Posterior Samples ROPE"
) +
facet_wrap(vars(df), scales="free_x") +
theme(legend.position = "bottom")
?brms::plot.brmsfit
tmp <- installed.packages()
installedpkgs <- as.vector(tmp[is.na(tmp[,"Priority"]), 1])
save(installedpkgs, file="installed_old.rda")
tmp <- installed.packages()
installedpkgs.new <- as.vector(tmp[is.na(tmp[,"Priority"]), 1])
missing <- setdiff(installedpkgs, installedpkgs.new)
library(tidyverse)
sessionInfo()
library(brms)
library(spotifyr)
install.packages("spotifyr")
library(rtweet)
install.packages("rtweet")
library(tidyverse)
library(rtweet)
bropen <- search_tweets("#bropenscience",
include_rts = FALSE)
View(bropen)
bropen <- search_tweets("#bropenscience",
n=3200,
include_rts = FALSE)
bropen <- search_tweets("#bropenscience",
n=3200,
include_rts = FALSE)
View(bropen)
bropen <- search_tweets("bropenscience",
n=3200,
include_rts = FALSE)
rstats <- search_tweets("rstats", n = 250)
bropen <- search_tweets("bropenscience",
n=200,)
bropen <- search_tweets("bropenscience", n=200)
remotes::install_github("LCBC-UiO/ggseg", build_vignettes = TRUE)
install.packages("cerebroViz")
install.packages("remotes")
remotes::install_github("LCBC-UiO/ggseg", build_vignettes = TRUE)
install.packages("devtools")
remotes::install_github("LCBC-UiO/ggseg", build_vignettes = TRUE)
remotes::install_github("LCBC-UiO/ggseg3d", build_vignettes = F)
library(ggseg)
library(ggseg3d)
ggseg()
?ggseg3d
ggseg3d()
ggseg3d::add_glassbrain()
library(tidyverse)
ggseg3d() + theme_void()
ggseg3d(surface="white")
ggseg3d(surface="inflated")
ggseg3d(surface="inflated", hemisphere = "left")
?ggseg3d
ggseg3d(surface="inflated", hemisphere = "left")
ggseg3d(surface="inflated", hemisphere = "left", palette = "Blues")
ggseg3d(surface="inflated", hemisphere = "left", palette = c("forestgreen", "white", "firebrick"))
library(RColorBrewer)
ggseg3d(atlas = dkt3d, colour = "p", text = "p")
ggseg3d(atlas = aseg3d,na.alpha= .5)%>%addglassbrain("left")%>%pancamera("left lateral")%>%removeaxes()
?ggseg3d_atlas
ggseg3d(atlas = dkt,na.alpha= .5)%>%addglassbrain("left")%>%pancamera("left lateral")%>%removeaxes()
p <- ggseg3d(atlas=dk_3d) %>%
remove_axes() %>%
pan_camera("right lateral")
p
ggseg3d(atlas = dk_3d,na.alpha= .5)%>%addglassbrain("left")%>%pancamera("left lateral")%>%removeaxes()
ggseg3d(atlas = dk_3d,na.alpha= .5)%>%add_glassbrain("left")%>%pancamera("left lateral")%>%removeaxes()
ggseg3d(atlas = dk_3d,na.alpha= .5)%>%add_glassbrain("left")%>%pan_camera("left lateral")%>%removeaxes()
ggseg3d(atlas = dk_3d,na.alpha= .5)%>%add_glassbrain("left")%>%pan_camera("left lateral")%>%remove_axes()
ggseg3d(atlas = aseg_3d,na.alpha= .5)%>%add_glassbrain("left")%>%pan_camera("left lateral")%>%remove_axes()
ggseg3d(atlas = aseg_3d,na.alpha= .5, palette = c("firebrick")) %>%
add_glassbrain("left")%>%
pan_camera("left lateral")%>%
remove_axes()
ggseg3d(atlas = dk_3d,na.alpha= .5, palette = c("firebrick")) %>%
add_glassbrain("left")%>%
pan_camera("left lateral")%>%
remove_axes()
library(ggseg)
install.packages("ggseg")
evts_s1 <- c(92.225,	104.225,	126.225,	182.225,	244.225,	258.225,	306.225,	76.225,	98.225,	112.225,	142.225,	200.225,	216.225,	234.225,	268.225,	318.225)
tr <- c(rep(2, 16))
evts_s1-tr
evts_s2-tr
evts_s2 <- c(99.65,	111.65,	133.65,	189.65,	251.65,	265.65,	313.65,	83.65,	105.65,	119.65,	149.65,	207.65,	223.65,	241.65,	275.65,	325.65)
evts_s2-tr
evts_s3 <- c(140.825,	152.825,	174.825,	230.825,	292.825,	306.825,	354.825, 124.825,	146.825,	160.825,	190.825,	248.825,	264.825,	282.825,	316.825,	366.825)
evts_s3-tr
evts_s4 <- c(246.725, 258.725, 280.725, 336.725, 398.725, 412.725, 460.725, 230.725, 252.725, 266.725, 296.725, 354.725, 370.725, 388.725, 422.725, 472.725)
evts_s4-tr
evts_s5 <- c(136.775, 148.775, 170.775, 226.775, 288.775, 302.775, 350.775, 120.775, 142.775, 156.775, 186.775, 244.775, 260.775, 278.775, 312.775, 362.775)
evts_s5-tr
evts_s12 <- c(92.975, 104.975, 126.975, 182.975, 244.975, 258.975, 306.975, 76.975, 98.975, 112.975, 142.975, 200.975, 216.975, 234.975, 268.975, 318.975)
evts_s12-tr
devtools::install_github("hadley/emo")
setwd("~/Dropbox/BANGOR/UNIVERSITY/PhD/R/R-tutorials/images")
setwd("~/Dropbox/BANGOR/UNIVERSITY/PhD/R/R-tutorials")
# install.packages("colorfindr")
# install.packages("tidyverse")
library(colorfindr) # extracts colors from images
library(tidyverse) # tidy data wrangling
